# rl-in-openai-gym

The code for Stanford's AA228/CS238 ("Decision making under uncertainty") final
project. Initial version runs all the supported algorithms one-by-one for the
'CartPole-v0' environment in the OpenAI Gym and plots average reward in the
tensorboard.

## Summary.

1.  Deep Q-learning algorithm [[1]](https://arxiv.org/abs/1312.5602)
2.  Experience replay [[1]](https://arxiv.org/abs/1312.5602)
3.  Target network [[1]](https://arxiv.org/abs/1312.5602)
4.  Gradient clipping [[1]](https://arxiv.org/abs/1312.5602)
5.  Target network with continuous update
    [[2]](https://arxiv.org/abs/1509.02971)
6.  Double Q-learning [[3]](https://arxiv.org/abs/1509.06461)

## Links.

1.  [Playing Atari with Deep Reinforcement
    Learning](https://arxiv.org/abs/1312.5602)
2.  [Continuous control with deep reinforcement
    learning](https://arxiv.org/abs/1509.02971)
3.  [Deep Reinforcement Learning with Double
    Q-learning](https://arxiv.org/abs/1509.06461)
